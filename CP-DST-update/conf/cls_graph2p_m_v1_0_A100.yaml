hydra:
  run:
    dir: ./

data_dir: multiwoz2.1_5
train_file: multiwoz2.1_5/train-5-full-value.tsv
dev_file: multiwoz2.1_5/dev-5-full-value.tsv
test_file: multiwoz2.1_5/test-5-full-value.tsv
ontology: multiwoz2.1_5/ontology-full.json
target_slot: "all"
domain_list:
train_single: "all"
reverse: False
max_seq_length: 128
max_turn_length: 22

model_name_or_path: pretrained-models/bert-base-uncased

task_name:

#output_dir: exp-multiwoz/v2.1.5.bert-base.graph2p
output_dir: exp-multiwoz/v2.1.5.bert-base.graph2p-m.v1.0.w1.A100.s${seed}
predict_dir:

num_workers: 4
prefetch_factor: 2


save_gate: True
model:
  _target_: models.BeliefTrackerShareSA_cls_graph2_plus_m.BeliefTracker.from_pretrained
  hidden_dim: 768
  attn_head: 12
  self_attention_type: 1
  extra_nbt: True
  value_embedding_type: "mean"
  dropout: 0.1
  fix_utterance_encoder: False
  fix_bert: False
  bert_path: pretrained-models/bert-base-uncased
  sa_add_layer_norm: True
  sa_add_residual: False
  sa_no_position_embedding: False
  override_attn: False
  share_position_weight: True
  extra_nbt_attn_head: 12
  override_attn_extra: False
  sa_act_1:
  diag_attn_hidden_scale: 1.0
  diag_attn_act:
  diag_attn_act_fn: "relu"
  mask_self: False
  inter_domain: False
  key_add_value: False
  key_add_value_pro: False
  mask_top_k: 0
  test_mode: -1
  graph_attn_type: 0
  graph_attn_head: 1
  graph_dropout: 0.1
  graph_add_output: False
  graph_add_residual: False
  graph_add_layer_norm: False
  fuse_type: 0
  fusion_no_transform: False
  fusion_act_fn: "gelu"
  slot_res:
  cls_type: 0
  distance_metric: product
  cls_loss_weight: 1.0
  graph_add_sup: 0
  graph_value_sup: 1.0
  transfer_sup: 0
  save_gate: ${save_gate}


pretrain:

do_train: True
evaluate_during_training: True

do_eval: True

per_gpu_train_batch_size: 12
per_gpu_dev_batch_size: 12
gradient_accumulation_steps: 2
fp16: True


learning_rate: 1e-4
num_train_epochs: 30
max_steps: -1
eval_steps: 1000
warmup_proportion: 0.1
warmup_steps:
max_grad_norm: 0.0
weight_decay: 0.01
adam_epsilon: 1e-6
adam_betas: "(0.9, 0.999)"
patience: 10

logging_steps: 1

# fairscale.FullyShardedDP
fairscale_config:
#  _target_: general_util.fsdp_utils.default_initialize
  _target_: general_util.fsdp_utils.recursive_initialize_remove_no_grad_module
  fp16: ${fp16}
  reshard_after_forward: False
  move_grads_to_cpu: False
  move_params_to_cpu: False
forward_sync: True

seed: 42
train_batch_size:
dev_batch_size:
eval_batch_size:
n_gpu:
local_rank: -1
world_size:
device:
no_cuda: False
